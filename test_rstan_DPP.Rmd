---
output: 
  pdf_document:
    fig_height: 4
    fig_width: 12
  md_document:
    variant: markdown_github
    fig_height: 4
    fig_width: 12
---

```{r knitr_options, include=FALSE}
library(knitr)
opts_chunk$set(fig.width=12, 
               fig.height=4,
               fig.path='RmdFigs/',
               warning=FALSE,
               message=FALSE,
               NULL)

set.seed(1235)
# install R packages if necessary:
if (!require("gtools")) 
  install.packages("gtools", repos="http://cran.rstudio.com")
if (!require("purrr")) 
  install.packages("purrr", repos="http://cran.rstudio.com")
if (!require("rstan")) 
  install.packages("rstan", repos="http://cran.rstudio.com")
if (!require('loo'))
  install.packages('loo', repos = 'http://cran.rstudio.com')
if (!require('ggplot2'))
  install.packages('ggplot2', repos = 'http://cran.rstudio.com')
if (!require('shinystan'))
  install.packages('shinystan', repos = 'http://cran.rstudio.com')
if (!require('tidyr'))
  install.packages('tidyr', repos = 'http://cran.rstudio.com')
if (!require('stringr'))
  install.packages('stringr', repos = 'http://cran.rstudio.com')

library(purrr)
library(rstan)
library(ggplot2)
library(stringr)
library(tidyr)
library(dplyr)
```


## Purpose

We're going to try out a few simple models using Dirichlet-process priors, to see how they work in Stan.

## Generating some fake data

Here we're going to simulate some data according to the [Chinese Resaurant Process]. This is described in this [blog post](http://blog.datumbox.com/the-dirichlet-process-the-chinese-restaurant-process-and-other-representations/) by [Vasilis Vryniotis](http://blog.datumbox.com/author/bbriniotis/).

```{r gen-data}

## function to add a customer to a table
add_customer <- function(thetas, alpha, base_function) {
  
  if (!exists('restaurant')) {
    ## start with an empty restaurant
    restaurant <<- data.frame(cust_id = integer(),
                         table_id = integer(),
                         theta = numeric(),
                         #value = numeric(),
                         NULL
                         )
    ## first customer sits at first table 
    cust <- 1
    table <- 1
    theta <- base_function()
  } else {
    # subsequent customers sometimes sit at new tables, sometimes join existing tables 
    num_prev_cust <- nrow(restaurant)
    num_curr_tables <- max(restaurant$table_id)
    cust <- num_prev_cust + 1
    prob_sit_new_table <- alpha / (num_prev_cust + alpha)
    sit_new_table <- rbinom(1,1,prob_sit_new_table)
    
    if (sit_new_table == 1) {
      table <- num_curr_tables + 1
      theta <- base_function()
    }
    else {
      table_counts <- restaurant %>% 
        group_by(table_id) %>%
        summarize(prob = n()/(num_prev_cust + alpha)
                  , theta = unique(theta)
                  ) %>%
        ungroup()
      selected_table <- table_counts %>% dplyr::filter(rmultinom(1, 1, table_counts$prob) == 1)
      table <- selected_table$table_id
      theta <- selected_table$theta
    }
  }  
    
  restaurant <<- restaurant %>%
    dplyr::bind_rows(list(cust_id = cust, table_id = table, theta = theta))
  return(TRUE)
}

# dispersion / parameter (alpha) - larger the value, more frequently new tables are spawned
a <- 1

## Base function used to generate new values of theta
g_base <- purrr::partial(rnorm, n = 1, mean = 10, sd = 5)


rm(restaurant)
for (i in 1:100) {
  add_customer(thetas = thetas, alpha = a, base_function = g_base)
}

## summarize
restaurant %>% 
  group_by(table_id) %>%
  summarize(n = n(),
            theta = unique(theta)
            )

```

Next we add noise to our model, to simulate a more realistic data-generating process 

```{r add-noise}

noise_generator <- purrr::partial(rnorm, n = 1, mean = 0, sd = 1)

restaurant <- restaurant %>%
  rowwise() %>%
  mutate(value = theta + noise_generator()) %>%
  ungroup()

ggplot(restaurant, aes(x = value, group = table_id, colour = factor(table_id))) + geom_density()

```

## Rescaling data 

Now, let's simulate the data analysis process which will be ignorant of true cluster IDs. 

First, we would rescale values according to observed mean & sd.

```{r rescale-data}
restaurant <- restaurant %>%
  mutate(global_mean = mean(value),
         global_sd = sd(value),
         rescaled_value = (value - global_mean)/global_sd
         )

ggplot(restaurant, aes(x = rescaled_value, group = table_id, colour = factor(table_id))) + geom_density()
```

## Defining the model

This is a first pass at fitting this model using Stan, as if it were a simple mixture model.

```{r model-code}

model_string <- "
data {
  int n;
  real y[n];
  int K; ## presumed number of groups
}

parameters {
  simplex[K] proportions;
  ordered[K] theta; ## means for each group
  real<lower=0> sigma; ## overall sigma; does not vary by group
}

model {
  real ps[K]; ## temp holder for log component densities
  
  ## prior on thetas; normally distributed with mean 0 and sd 1
  theta ~ normal(0, 1);
  
  ## for now, no prior on proportions. Should prob be a dirichlet prior
  for (i in 1:n) {
    for (k in 1:K) {
      ps[k] <- log(proportions[k]) + normal_log(y[i], theta[k], sigma);
    }
    increment_log_prob(log_sum_exp(ps));
  }
}

"

```

Note that we've skipped the "generated quantities" block for now; means we won't be able to do PP checks or run loo. We'll add it back in once we have a model that works well.

## Fitting the model in Stan

```{r fit-model}
iter <- 500
stan_samples <- stan(model_code = model_string,
                     iter = iter,
                     data = list(y = restaurant$rescaled_value,
                                 n = nrow(restaurant),
                                 K = 5),
                     model_name = 'Simple mixture model with K = 5'
                     )

# summarize outcome of interest
print(stan_samples, pars = 'theta')
```

### Evaluating model fit 

Review traceplot for theta - main parameter of interest 

```{r eval-traceplot}
traceplot(stan_samples, pars = 'theta')
```

### Review 50 & 95% posterior density for theta

```{r eval-theta}
plot(stan_samples, pars = 'theta')
```

### Comparing estimated theta to true values

Next, since we are dealing with simulated data, we can compare the values of our estimated parameter (*theta*) to the true value used to generate the samples. 

It's unlikely that the true value would be well outside our posterior distribution for theta, but it's possible.

First we do some data-prep to collect & transform the values of theta toput them on the original scale.

```{r summarize-theta}

## true values, on original scale
true_thetas <- unique(restaurant$theta)
true_thetas <- true_thetas[order(true_thetas)]

## save, so we can transform thetahat to original scale
global_mean <- unique(restaurant$global_mean)
global_sd <- unique(restaurant$global_sd)

# matrix of estimated values, on modified (rescaled) scale
thetahat <- unlist(rstan::extract(stan_samples, 'theta')$theta)

# modify into a dataset
#rownames(thetahat) <- seq(from = 1, to = nrow(thetahat), by = 1)
colnames(thetahat) <- seq(from = 1, to = 5, by = 1)
thdata <- as.data.frame(thetahat) %>%
  mutate(iter = n()) %>%
  tidyr::gather(sample, mean_rescaled, 1:5, convert = T) %>%
  mutate(mean_original = (mean_rescaled * global_sd) + global_mean)
```

Next, we plot the observed & true means on the recentered scale

```{r plot-true-vs-observed-recentered}
library(ggplot2)
ggplot() +
  geom_density(aes(x = mean_rescaled, group = sample, colour = 'estimated theta'), data = thdata) +
  geom_vline(aes(xintercept = mean, colour = 'true theta values'), data = restaurant %>% group_by(table_id) %>% summarize(mean = mean(rescaled_value, na.rm = T)) %>% ungroup())
```

And, the observed vs true estimates of theta

```{r plot-true-vs-observed}
library(ggplot2)
ggplot() +
  geom_density(aes(x = mean_original, group = sample, colour = 'estimated theta'), data = thdata) +
  geom_vline(aes(xintercept = true_thetas, colour = 'true theta values'))
```

We notice that even though we have constrained the values of theta to be ordered, there are still some label-switching problems. 

IE each 'sample' has a multi-modal distribution of theta. Collapsing these may yield a more complete picture.

```{r plot-true-vs-observed-theta}
library(ggplot2)
ggplot() +
  geom_density(aes(x = mean_original, colour = 'estimated theta'), data = thdata) +
  geom_vline(aes(xintercept = true_thetas, colour = 'true theta values'))
```

Notice how the smoothed estimates look more consistent with a 3-cluster result (ie K = 3), rather than the true value (K = 5).

Let's see if Loo can help us distinguish between different values of K.

## Using Loo to find optimal values of K

[Loo](http://www.stat.columbia.edu/~gelman/research/unpublished/loo_stan.pdf) approximates leave-one-out validation, and can be used to identify observations with undue influence (leverage) on the model and/or for model comparison. 

In this case, we will use Loo for sanity checking & to compare model fit under different values of K.

### Writing the generated quantities block

In order to use Loo, we need to first calculate the log_liklihood in the generated quantities block.

Our revised model string will look like this: 

```{r model-definition-with-genquant}
model_string_with_loglik <- "
data {
  int n;
  real y[n];
  int K; ## presumed number of groups
}

parameters {
  simplex[K] proportions;
  ordered[K] theta; ## means for each group
  real<lower=0> sigma; ## overall sigma; does not vary by group
}

model {
  ## prior on thetas; normally distributed with mean 0 and sd 1
  theta ~ normal(0, 1);
  
  ## for now, no prior on proportions. Should prob be a dirichlet prior
  for (i in 1:n) {
    real ps[K]; ## temp holder for log component densities
    for (k in 1:K) {
      ps[k] <- log(proportions[k]) + normal_log(y[i], theta[k], sigma);
    }
    increment_log_prob(log_sum_exp(ps));
  }
}

generated quantities {
  real log_lik[n];
  
  for (i in 1:n) {
    real ps[K]; ## temp holder for log component densities
    for (k in 1:K) {
      ps[k] <- log(proportions[k]) + normal_log(y[i], theta[k], sigma);
    }
    log_lik[i] <- log_sum_exp(ps);
  }
}
"
```

Let's do a quick sanity check to see if the model works as it did before.

```{r sanity-check-post-log-lik}
stan_samples_k5 <- stan(model_code = model_string_with_loglik,
                     iter = iter,
                     data = list(y = restaurant$rescaled_value,
                                 n = nrow(restaurant),
                                 K = 5),
                     model_name = 'Simple mixture model with K = 5 (v2)'
                     )

# summarize outcome of interest
print(stan_samples_k5, pars = 'theta')
```

### Loo with k = 5 

```{r loo-sample-k5}
loo_k5 <- loo(rstan::extract(stan_samples_k5, 'log_lik')$log_lik)
```

### Estimating model with k = 3,4 and 5

(note: output is hidden for sake of brevity)

```{r stan-example-k3, collapse=TRUE, results = 'hide'}
## K = 2
stan_samples_k2 <- stan(model_code = model_string_with_loglik,
                     iter = iter,
                     data = list(y = restaurant$rescaled_value,
                                 n = nrow(restaurant),
                                 K = 2),
                     model_name = 'Simple mixture model with K = 2 (v2)'
                     )

print(stan_samples_k2, pars = 'theta')
loo_k2 <- loo(rstan::extract(stan_samples_k2, 'log_lik')$log_lik)

## K = 3
stan_samples_k3 <- stan(model_code = model_string_with_loglik,
                     iter = iter,
                     data = list(y = restaurant$rescaled_value,
                                 n = nrow(restaurant),
                                 K = 3),
                     model_name = 'Simple mixture model with K = 3 (v2)'
                     )

print(stan_samples_k3, pars = 'theta')
loo_k3 <- loo(rstan::extract(stan_samples_k3, 'log_lik')$log_lik)

## K = 4
stan_samples_k4 <- stan(model_code = model_string_with_loglik,
                     iter = iter,
                     data = list(y = restaurant$rescaled_value,
                                 n = nrow(restaurant),
                                 K = 4),
                     model_name = 'Simple mixture model with K = 4 (v2)'
                     )

# summarize outcome of interest
print(stan_samples_k4, pars = 'theta')
loo_k4 <- loo(rstan::extract(stan_samples_k4, 'log_lik')$log_lik)

```

### Comparing model fit using Loo

Note that higher values of elpd_loo are better -- so the first model (loo_k4) listed here is likely the "best"" fit.

```{r compare-model-fit-loo}
loo::compare(loo_k2, loo_k3, loo_k4, loo_k5)
```

The relative differences between the models with k = 3, 4, and 5 are fairly small, however.

The biggest gain is seen between models with K = 2 vs K = 3: 

```{r compare-k2-vs-k3}
loo::compare(loo_k2, loo_k3)
```

Looking at K = 3 vs K = 4, however, the difference is minimal:

```{r compare-k3-vs-k4}
loo::compare(loo_k3, loo_k4)
```

And, actually worsens for K = 4 vs K = 5

```{r compare-k4-vs-k5}
loo::compare(loo_k4, loo_k5)
```

Based on these results, the best fits have K = 3 or K = 4. Ideal result will likely be an average of the two models.

## Improving the model using Dirichlet prior 

Now that we have a reasonably-well-fitting mixture model, let's see how this can be improved by incorporating a Dirichlet prior.

Since our data were generated according to a dirichlet process (at least we think they were!), it's plausible that this prior may yield a distribution of group sizes that better match our data.

```{r model-definition-with-DPP}
model_string_with_dp <- "
data {
  int n;
  real y[n];
  int K; ## presumed number of groups
}

parameters {
  simplex[K] proportions;
  ordered[K] theta; ## means for each group
  real<lower=0> sigma; ## overall sigma; does not vary by group
  vector<lower=0>[K] alpha;
}

model {
  ## prior on thetas; normally distributed with mean 0 and sd 1
  theta ~ normal(0, 1);
  proportions ~ dirichlet(alpha);
  
  ## for now, no prior on proportions. Should prob be a dirichlet prior
  for (i in 1:n) {
    real ps[K]; ## temp holder for log component densities
    for (k in 1:K) {
      ps[k] <- log(proportions[k]) + normal_log(y[i], theta[k], sigma);
    }
    increment_log_prob(log_sum_exp(ps));
  }
}

generated quantities {
  real log_lik[n];
  
  for (i in 1:n) {
    real ps[K]; ## temp holder for log component densities
    for (k in 1:K) {
      ps[k] <- log(proportions[k]) + normal_log(y[i], theta[k], sigma);
    }
    log_lik[i] <- log_sum_exp(ps);
  }
}
"
```

### Fitting Dirichlet-prior model to data 

Try fitting this revised model to our data. For now, start with K = 5 since that's the "ground truth"
```{r, eval = T}
stan_samples_dp <- stan(model_code = model_string_with_dp,
                     iter = iter,
                     data = list(y = restaurant$rescaled_value,
                                 n = nrow(restaurant),
                                 K = 5),
                     model_name = 'Simple mixture model with Dirichlet prior (K = 5)'
                     )

print(stan_samples_dp, pars = 'theta')

## Note: increasing adapt_delta to see if this helps
stan_samples_dp2 <- stan(model_code = model_string_with_dp,
                     iter = iter,
                     data = list(y = restaurant$rescaled_value,
                                 n = nrow(restaurant),
                                 K = 5),
                     model_name = 'Simple mixture model with Dirichlet prior (K = 5)',
                     control = list(adapt_delta = 0.9)
                     )

# summarize outcome of interest
print(stan_samples_dp2, pars = 'theta')


# Note: increasing max_treedepth to see if this helps 
stan_samples_dp3 <- stan(model_code = model_string_with_dp,
                     iter = iter,
                     data = list(y = restaurant$rescaled_value,
                                 n = nrow(restaurant),
                                 K = 5),
                     model_name = 'Simple mixture model with Dirichlet prior (K = 5)',
                     control = list(adapt_delta = 0.9, max_treedepth = 15)
                     )

# summarize outcome of interest
print(stan_samples_dp3, pars = 'theta')

```

Before we go any further, it appears our Rhat values are pretty crappy (ie not close to 1).  Let's increase the number of iterations to see how this improves the model fit.

### compare dp-derived model to the simple mixture model

```{r compare-dp-to-simple-model, eval = T}

loo_dp <- loo(rstan::extract(stan_samples_dp3,'log_lik')$log_lik)
loo::compare(loo_dp, loo_k5)

```

