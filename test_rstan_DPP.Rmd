---
output: 
  pdf_document:
    fig_height: 4
    fig_width: 12
    toc: yes
    toc_depth: 1
  md_document:
    variant: markdown_github
    fig_height: 6
    fig_width: 12
    toc: yes
    toc_depth: 1
---

```{r knitr_options, include=FALSE}
library(knitr)
opts_chunk$set(fig.width=12, 
               fig.height=4,
               fig.path='RmdFigs/',
               warning=FALSE,
               message=FALSE,
               NULL)

# install R packages if necessary:
if (!require("gtools")) 
  install.packages("gtools", repos="http://cran.rstudio.com")
if (!require("purrr")) 
  install.packages("purrr", repos="http://cran.rstudio.com")
if (!require("rstan")) 
  install.packages("rstan", repos="http://cran.rstudio.com")
if (!require('loo'))
  install.packages('loo', repos = 'http://cran.rstudio.com')
if (!require('ggplot2'))
  install.packages('ggplot2', repos = 'http://cran.rstudio.com')
if (!require('shinystan'))
  install.packages('shinystan', repos = 'http://cran.rstudio.com')
if (!require('tidyr'))
  install.packages('tidyr', repos = 'http://cran.rstudio.com')
if (!require('stringr'))
  install.packages('stringr', repos = 'http://cran.rstudio.com')
if (!require('parallel'))
  install.packages('parallel', repos = 'http://cran.rstudio.com')

library(purrr)
library(rstan)
library(ggplot2)
library(stringr)
library(tidyr)
library(dplyr)
options(mc.cores = max(parallel::detectCores(),4))
```


## Purpose

We're going to try out a few simple models using Dirichlet-process priors, to see how they work in Stan.

Note that the first version of this document used a particular / fixed seed to compare methods for a particular set of simulated data. This revised version is going to compare the methods for a number of iterations, since the best method *on average* may not be the same as the best method for a particular set of data.

## Generating some fake data

Here we're going to simulate some data according to the [Chinese Resaurant Process](https://en.wikipedia.org/wiki/Chinese_restaurant_process). This is described in this [blog post](http://blog.datumbox.com/the-dirichlet-process-the-chinese-restaurant-process-and-other-representations/) by [Vasilis Vryniotis](http://blog.datumbox.com/author/bbriniotis/).

```{r gen-data-function}

## function to add a customer to a table
add_customer <- function(alpha, base_function, d = NULL) {
  
  if (is.null(d)) {
    ## start with an empty restaurant
    d <- data.frame(cust_id = integer(),
                         table_id = integer(),
                         theta = numeric(),
                         #value = numeric(),
                         NULL
                         )
    ## first customer sits at first table 
    cust <- 1
    table <- 1
    theta <- base_function()
    
  } else {
    # subsequent customers sometimes sit at new tables, sometimes join existing tables 
    num_prev_cust <- nrow(d)
    num_curr_tables <- max(d$table_id)
    cust <- num_prev_cust + 1
    prob_sit_new_table <- alpha / (num_prev_cust + alpha)
    sit_new_table <- rbinom(1, 1, prob_sit_new_table)
    
    if (sit_new_table == 1) {
      ## add new table
      table <- num_curr_tables + 1
      theta <- base_function()
    }
    else {
      ## pick from existing tables
      table_counts <- d %>% 
        group_by(table_id) %>%
        summarize(prob = unique(n()/(num_prev_cust + alpha))
                  , theta = unique(theta)
                  ) %>%
        ungroup()
      selected_table <- 
        table_counts %>% 
        dplyr::filter(rmultinom(1, 1, table_counts$prob) == 1)
      table <- selected_table$table_id
      theta <- selected_table$theta
    }
  }  
  ## add customer to dataset
  d <- d %>%
    dplyr::bind_rows(list(cust_id = cust, table_id = table, theta = theta))
  return(d)
}

## code to generate data - wrap in a function so we can use this later.
simulate_data <- 
  function(a = 1
           , g_base = purrr::partial(rnorm, n = 1, mean = 10, sd = 5)
           , n = 100
  ) {
    d <- NULL
    for (i in seq(from = 1, to = n, by = 1)) {
      d <- 
        add_customer(alpha = a
                     , base_function = g_base
                     , d = d
                     )
    }
    return(d)
  }
```

Now that we've defined the function to simulate the data, let's generate our first dataset.

We will use this to build & test the various models, then repeat the process with a variety of seeds to see which method is best on average. 

```{r sim-data}

set.seed(123)
restaurant <- simulate_data()

## summarize
restaurant %>% 
  group_by(table_id) %>%
  summarize(n = n(),
            theta = unique(theta)
            )

```

Next we add noise to our model, to simulate a more realistic data-generating process 

```{r add-noise}

noise_generator <- purrr::partial(rnorm, n = 1, mean = 0, sd = 1)

restaurant <- restaurant %>%
  rowwise() %>%
  mutate(value = theta + noise_generator()) %>%
  ungroup()

ggplot(restaurant, aes(x = value, group = table_id, colour = factor(table_id))) + geom_density()

```


## Rescaling data 

Now, let's simulate the data analysis process which will be ignorant of true cluster IDs. 

First, we would rescale values according to observed mean & sd.

```{r rescale-data}
restaurant <- restaurant %>%
  mutate(global_mean = mean(value)
         , global_sd = sd(value)
         , rescaled_value = (value - global_mean)/global_sd
         , theta_scaled = (theta - global_mean)/global_sd
         )

ggplot(restaurant
       , aes(x = rescaled_value
             , group = table_id
             , fill = factor(table_id)
             , colour = factor(table_id))
       ) + 
  geom_histogram(position = 'dodge') +
  theme_bw() +
  ggtitle('Sampled values by table (cluster) id, after rescaling')
```

### Writing prep-data function

Let's wrap these last two steps as functions & append them to our simulate_data function. We'll call the final function **prep_restaurant_data**.

```{r }

## first define the function to add noise to the model
add_noise <- 
  function(d
           , noise_generator = purrr::partial(rnorm, n = 1, mean = 0, sd = 1)
           , ...
  ) {
    d <- d %>%
      rowwise() %>%
      mutate(value = theta + noise_generator()) %>%
      ungroup()
    return(d)
  }

## next define a function to rescale the estimated values 
rescale_values <- 
  function(d
           , ...
  ) {
    d <- d %>%
      mutate(global_mean = mean(value)
             , global_sd = sd(value)
             , rescaled_value = (value - global_mean)/global_sd
             , theta_scaled = (theta - global_mean)/global_sd
      )
    return(d)
  }

prep_restaurant_data <- purrr::compose(rescale_values, add_noise, simulate_data)

## test the process
testthat::test_that(
  'prep_restaurant_data returns dataset'
  , {
    test_rest <- prep_restaurant_data()
    testthat::expect_is(test_rest, 'data.frame')
  })

testthat::test_that(
  'running prep_restaurant_data twice yields different datasets'
  , {
    test_rest <- prep_restaurant_data()
    test_rest2 <- prep_restaurant_data()
    testthat::expect_false(isTRUE(all.equal(test_rest, test_rest2)))
  }
)

testthat::test_that(
  'prep_restaurant_data returns correct number of rows'
  , {
    test_rest <- prep_restaurant_data(n = 100)
    testthat::expect_equal(nrow(test_rest), 100)
    test_rest2 <- prep_restaurant_data(n = 500)
    testthat::expect_equal(nrow(test_rest2), 500)
  }
)
```

We will come back to this function later in our code. For now, we will use the simulated data to define the models we want to use to segment these data.

## Defining the mixture model

As a first pass at fitting this model using Stan, we will define a simple Gaussian mixture model.

```{r model-code}

model_string <- "
data {
  int n;
  real y[n];
  int K; ## presumed number of groups
}

parameters {
  simplex[K] proportions;
  ordered[K] theta; ## means for each group
  real<lower=0> sigma; ## overall sigma; does not vary by group
}

model {
  real ps[K]; ## temp holder for log component densities
  
  ## prior on thetas; normally distributed with mean 0 and sd 1
  theta ~ normal(0, 1);
  
  ## for now, no prior on proportions. Should prob be a dirichlet prior
  for (i in 1:n) {
    for (k in 1:K) {
      ps[k] <- log(proportions[k]) + normal_log(y[i], theta[k], sigma);
    }
    increment_log_prob(log_sum_exp(ps));
  }
}

"

```

Note that we've skipped the "generated quantities" block for now; this means we won't be able to do PP checks or run loo. 

We'll add it back in once we see how well this model works.

## Fitting the model in Stan

```{r fit-model}
iter <- 500
seed <- 1235 ## seed value passed to Stan
stan_samples <- stan(model_code = model_string,
                     iter = iter,
                     seed = seed,
                     data = list(y = restaurant$rescaled_value,
                                 n = nrow(restaurant),
                                 K = 5),
                     model_name = 'Gaussian mixture model'
                     )

# summarize outcome of interest
print(stan_samples, pars = 'theta')
```

### Evaluating model fit 

Review traceplot for theta - main parameter of interest 

```{r eval-traceplot}
traceplot(stan_samples, pars = 'theta')
```

### Review 50 & 95% posterior density for theta

```{r eval-theta}
plot(stan_samples, pars = 'theta')
```

### Comparing estimated theta to true values

Next, since we are dealing with simulated data, we can compare the values of our estimated parameter (*theta*) to the true value used to generate the samples. 

It's unlikely that the true value would be well outside our posterior distribution for theta, but it's possible.

First we do some data-prep to collect & transform the values of theta toput them on the original scale.

```{r summarize-theta}

## true values, on original scale
true_thetas <- unique(restaurant$theta)

## save, so we can transform thetahat to original scale
global_mean <- unique(restaurant$global_mean)
global_sd <- unique(restaurant$global_sd)

# matrix of estimated values, on modified (rescaled) scale
thetahat <- unlist(rstan::extract(stan_samples, 'theta')$theta)

# modify into a dataset
#rownames(thetahat) <- seq(from = 1, to = nrow(thetahat), by = 1)
colnames(thetahat) <- seq(from = 1, to = 5, by = 1)
thdata <- as.data.frame(thetahat) %>%
  mutate(iter = n()) %>%
  tidyr::gather(sample, mean_rescaled, 1:5, convert = T) %>%
  mutate(mean_original = (mean_rescaled * global_sd) + global_mean)
```

Next, we plot the observed & true means on the recentered scale

```{r plot-true-vs-observed-recentered}
library(ggplot2)
ggplot() +
  geom_density(aes(x = mean_rescaled, group = sample, colour = 'estimated theta')
               , data = thdata) +
  geom_vline(aes(xintercept = mean, colour = 'true theta values')
             , data = 
               restaurant %>% 
               group_by(table_id) %>% 
               summarize(mean = mean(rescaled_value, na.rm = T)) %>% 
               ungroup()
             ) +
  ggtitle('Estimated & actual values of theta, by estimated cluster id')
```

From this plot, we can see that there is some "class switching" in play. 

Even though we've constrained the clusters to be ordered so that the cluster with the lowest theta will usually be listed first, we do see that some clusters are bimodal.

We also can't tell whether this model has estimated the relative size of each cluster correctly, since the estimates for the "proportion" parameter aren't displayed here.

Before we go any further, let's improve this plot to make it mode demonstrative. As before, we'll wrap this into a set of functions so we can re-use the code in subsequent model runs.

The first utility function we need will extract a parameter value from the stanfit object & prepare it for plotting. We will make it generic so that it can be reused.

```{r get-sampled-parameter-fun}

get_sampled_parameter <- 
  function(stanfit
           , pars
           , parameter_value = 'parameter_value'
           , parameter_id = 'parameter_id'
  ) {
    
  estimated_pars <- 
    rstan::extract(object = stanfit, pars = pars)[[1]] %>%
    as.data.frame(.)
  
  names(estimated_pars) <- seq_len(ncol(estimated_pars))
  
  vars_to_rename <- list('parameter_id', 'parameter_value')
  names(vars_to_rename) <- c(parameter_id, parameter_value)
  
  estimated_pars %>%
    mutate(iter = 1:n()) %>%
    gather(parameter_id, parameter_value, -iter, convert = T) %>%
    rename_(.dots = vars_to_rename)
}
```

Let's use this to plot our estimates of theta according to their estimated proportion. We will compare this to the true cluster means & proportions.

```{r compare-k5-estimates-theta}

## weighted distribution of estimated values of theta from our model
estimated_theta <- get_sampled_parameter(stan_samples 
                                         , pars = 'theta'
                                         , parameter_id = 'cluster_id'
                                         , parameter_value = 'theta_scaled'
                                         )

estimated_prop <-  get_sampled_parameter(stan_samples
                                         , pars = 'proportions'
                                         , parameter_id = 'cluster_id'
                                         , parameter_value = 'prop'
                                         )

estimated_data <- 
  estimated_theta %>%
  inner_join(estimated_prop
             , by = c('cluster_id','iter')
             )
```

We now have a data frame with the assigned cluster_id, proportion & location for each draw from the posterior density.

```{r review-estimates-k5-theta}
head(estimated_data)
```

Let's plot these in two-dimensions

```{r plot-estimates-k5-theta}
## plot locations & proportion irrespective of cluster id
(plot <- 
  ggplot() +
  geom_density_2d(data = estimated_data
                  , mapping = aes(x = theta_scaled
                                  , y = prop
                                  , colour = 'estimated - mixture model'
                                  , fill = 'estimated - mixture model')
                  ) + 
  scale_fill_discrete(guide = "none") +
  scale_size_continuous(guide = "none") +
  theme_bw()
)
```

Finally, we can overlay the location & proportions of true clusters. Note that these are scaled according to their relative size.

```{r plot-estimates-k5-theta-2}

## weighted distribution of true values of theta
true_theta <- 
  restaurant %>%
  mutate(theta_scaled = (theta - global_mean)/global_sd
         , global_n = n()
         ) %>%
  group_by(table_id) %>%
  summarize(theta_scaled = unique(theta_scaled)
            , prop = unique(n()/global_n)
            ) %>%
  ungroup()

plot + 
  geom_point(data = true_theta
             , mapping = aes(x = theta_scaled, y = prop, size = prop, colour = 'true clusters', fill = 'true clusters')
             ) +
  ggtitle('Location & size of estimated clusters, compared to true values')

```

Depending on which draw of the random simulation we are looking at, this may or may not fit our clusters well.

Let's see if Loo can help us distinguish between different values of K.

## Using Loo to find optimal values of K

[Loo](http://www.stat.columbia.edu/~gelman/research/unpublished/loo_stan.pdf) approximates leave-one-out validation, and can be used to identify observations with undue influence (leverage) on the model and/or for model comparison. 

In this case, we will use Loo for sanity checking & to compare model fit under different values of K.

### Adding the generated quantities block

In order to use Loo, we need to first calculate the log_liklihood in the generated quantities block.

Our revised model string will look like this: 

```{r model-definition-with-genquant}
model_string_with_loglik <- "
data {
  int n;
  real y[n];
  int K; ## presumed number of groups
}

parameters {
  simplex[K] proportions;
  ordered[K] theta; ## means for each group
  real<lower=0> sigma; ## overall sigma; does not vary by group
}

model {
  ## prior on thetas; normally distributed with mean 0 and sd 1
  theta ~ normal(0, 1);
  
  ## for now, no prior on proportions. Should prob be a dirichlet prior
  for (i in 1:n) {
    real ps[K]; ## temp holder for log component densities
    for (k in 1:K) {
      ps[k] <- log(proportions[k]) + normal_log(y[i], theta[k], sigma);
    }
    increment_log_prob(log_sum_exp(ps));
  }
}

generated quantities {
  real log_lik[n];
  
  for (i in 1:n) {
    real ps[K]; ## temp holder for log component densities
    for (k in 1:K) {
      ps[k] <- log(proportions[k]) + normal_log(y[i], theta[k], sigma);
    }
    log_lik[i] <- log_sum_exp(ps);
  }
}
"
```

Let's do a quick sanity check to see if the model works as it did before.

```{r sanity-check-post-log-lik}
stan_samples_k5 <- stan(model_code = model_string_with_loglik,
                     iter = iter,
                     seed = seed,
                     data = list(y = restaurant$rescaled_value,
                                 n = nrow(restaurant),
                                 K = 5),
                     model_name = 'Simple mixture model with K = 5 (v2)'
                     )

# summarize outcome of interest
print(stan_samples_k5, pars = 'theta')
```

### Loo with k = 5 

```{r loo-sample-k5}
loo_k5 <- loo(rstan::extract(stan_samples_k5, 'log_lik')$log_lik)
```

### Estimating model with k = 3, 4, and 5

(note: output is hidden for sake of brevity)

```{r stan-example-k3, collapse=TRUE, results = 'hide'}
## K = 2
stan_samples_k2 <- stan(model_code = model_string_with_loglik
                        , iter = iter
                        , seed = seed
                        , data = list(y = restaurant$rescaled_value,
                                      n = nrow(restaurant),
                                      K = 2)
                        )

print(stan_samples_k2, pars = 'theta')
loo_k2 <- loo(rstan::extract(stan_samples_k2, 'log_lik')$log_lik)

## K = 3
stan_samples_k3 <- stan(model_code = model_string_with_loglik
                        , iter = iter
                        , seed = seed
                        , data = list(y = restaurant$rescaled_value,
                                      n = nrow(restaurant),
                                      K = 3)
                        )

print(stan_samples_k3, pars = 'theta')
loo_k3 <- loo(rstan::extract(stan_samples_k3, 'log_lik')$log_lik)

## K = 4
stan_samples_k4 <- stan(model_code = model_string_with_loglik
                        , iter = iter
                        , seed = seed
                        , data = list(y = restaurant$rescaled_value,
                                      n = nrow(restaurant),
                                      K = 4)
                        )

# summarize outcome of interest
print(stan_samples_k4, pars = 'theta')
loo_k4 <- loo(rstan::extract(stan_samples_k4, 'log_lik')$log_lik)

```

### Comparing model fit using Loo

Note that higher values of elpd_loo are better -- so the first model listed here is likely the "best" fit.

```{r compare-model-fit-loo}
loo::compare(loo_k2, loo_k3, loo_k4, loo_k5)
```

Depending on the data, we might see a "best fit" with k = 3, 4, or 5. In most cases, the relative difference is likely to be small.

To get a sense of whether these differences are "significant", we compare any two models. 

For example, K = 2 vs K = 3: 

```{r compare-k2-vs-k3}
loo::compare(loo_k2, loo_k3)
```

and, K = 3 vs K = 4:

```{r compare-k3-vs-k4}
loo::compare(loo_k3, loo_k4)
```

finally K = 4 vs K = 5:

```{r compare-k4-vs-k5}
loo::compare(loo_k4, loo_k5)
```

In practice, Loo can be useful for model comparison -- ie picking the model that best fits the data. In practice, we would likely want to combine results from several models, weighted according to their relative elpd_loo values.

## Improving the model using Dirichlet prior 

Now that we have a reasonably-well-fitting mixture model, let's see whether this can be improved by incorporating a Dirichlet (process) prior. This is not a pure non-parametric implementation, since we are specifying the max possible number of groups (K) but it should get us close.

Since our data were generated according to a dirichlet process (at least we think they were!), it's plausible that this model would yield group sizes that better match our data.


```{r define-dp-model-with-DPP}
model_string_with_dp <- "
data {
  int n;
  real y[n];
  int K; ## presumed number of groups
}

parameters {
  simplex[K] proportions;  ## mixing proportion (pis)
  ordered[K] theta;        ## means for each group
  real<lower=0> sigma;     ## overall sigma; does not vary by group
  real<lower=0> a;         ## concentration parameter
}

transformed parameters {
  vector<lower=0>[K] alpha;
  for (k in 1:K) {
    alpha[k] <- a/k;
  }
}

model {
  ## priors on parameters
  theta ~ normal(0, 1);
  a ~ normal(10,10);
  proportions ~ dirichlet(alpha);
  
  ## compute likelihood for each observation, integrating out discrete-ness of groups
  for (i in 1:n) {
    real ps[K]; ## temp holder for log component densities
    for (k in 1:K) {
      ps[k] <- log(proportions[k]) + normal_log(y[i], theta[k], sigma);
    }
    increment_log_prob(log_sum_exp(ps));
  }
}

generated quantities {
  real log_lik[n];
  
  for (i in 1:n) {
    real ps[K]; ## temp holder for log component densities
    for (k in 1:K) {
      ps[k] <- log(proportions[k]) + normal_log(y[i], theta[k], sigma);
    }
    log_lik[i] <- log_sum_exp(ps);
  }
}
"
```

### Fitting Dirichlet-prior model to data 

Try fitting this revised model to our data. For now, start with K = 5.

```{r fit-dp-model-k5, eval = T}
stan_samples_dp <- stan(model_code = model_string_with_dp
                        , iter = iter*2
                        , seed = seed
                        , data = list(y = restaurant$rescaled_value,
                                      n = nrow(restaurant),
                                      K = 5)
                        , control = list(adapt_delta = 0.99, max_treedepth = 15)
                        , model_name = 'Gaussian mixture model with Dirichlet prior'
                        )

## look at thetas
print(stan_samples_dp, pars = c('theta'))

```

Here I should mention that we increased the number of iterations, and increased the values of adapt_delta & max_treedepth. These settings were determined through trial and error for the sample of random data being considered. For the particular sample used here, the fit may be different.

### compare dp-derived model to the simple mixture model

Comparing the two models using *loo* may show some improvement in leave-one-out predictive performance. However, for a robust comparison we would want to run these two models for many draws of random data (which we will do later). 

First, we need to fine tune the DP model.

```{r compare-dp-to-simple-model, eval = T}

loo_dp <- loo(rstan::extract(stan_samples_dp,'log_lik')$log_lik)
loo::compare(loo_dp, loo_k4, loo_k5)

loo::compare(loo_dp, loo_k4)

loo::compare(loo_dp, loo_k5)

```

Now we want to see how well the estimated group-level parameters approximate the true values.

First, we will write this model-evaluating code into a function so we can reuse it later.

```{r function-plot-thetas}
#' Function to plot estimated mean & size of clusters 
#' compared to true values.
#' 
#' @returns ggplot grob
#' 
#' @param stanfit stanfit objects (optionally, named vector of stanfit objects)
#' @param d data containing "true values" for the parameter for each input obs
#' @param stanfit_loc_var parameter name for cluster location
#' @param stanfit_size_var parameter name for cluster proportion
#' @param d_cluster_id var name in d for cluster identifier
#' @param d_cluster_val var name in d for cluster-level mean (parameter not value)
plot_cluster_parameters <- 
  function(stanfit
           , d
           , stanfit_loc_var = 'theta'
           , stanfit_size_var = 'proportions'
           , d_cluster_id = 'table_id'
           , d_cluster_val = 'theta_scaled'
  ) {
    ## initialize plot object 
    p <- ggplot() + 
      scale_fill_discrete(guide = "none") +
      scale_size_continuous(guide = "none") +
      theme_bw() +
      ggtitle('Location & size of estimated clusters vs true values')
    
    ## summarize true values 
    true_values <- d %>%
      mutate(total_n = n()) %>%
      group_by_(d_cluster_id) %>%
      mutate(prop = n()/total_n) %>%
      summarize_each(funs = funs(unique), matches(d_cluster_val), prop) %>%
      ungroup() %>%
      mutate(source = 'true clusters')
    
    ## add summarized true values to plot
    p <- p +
      geom_point(data = true_values
                 , mapping = aes_string(x = d_cluster_val, y = 'prop', size = 'prop'
                                 , colour = 'source'
                                 , fill = 'source'
                                 )
                 ) 
    
    ## force stanfit objects into vector
    if (length(stanfit) == 1) {
      stanfit <- c('model' = stanfit)
    }
    ## apply default naming to vector
    if (is.null(names(stanfit))) {
      names(stanfit) <- paste('model ',seq_len(length(stanfit)))
    }
    plotdata <- list() ## list of plotdata objects
    
    ## for each stanfit object, summarize fit of clusters & add to plot
    for (sf in seq_len(length(stanfit))) {
      plotdata[[sf]] <- 
        get_sampled_parameter(stanfit[[sf]]
                              , pars = stanfit_loc_var
                              , parameter_value = d_cluster_val
                              , parameter_id = d_cluster_id
                              ) %>%
        inner_join(get_sampled_parameter(stanfit[[sf]]
                                         , pars = stanfit_size_var
                                         , parameter_value = 'prop'
                                         , parameter_id = d_cluster_id
                                         )
                   , by = c(d_cluster_id, 'iter')
                   ) %>%
        mutate(source = str_c('estimates from ',names(stanfit)[[sf]])) 
      
      ## add data to plot
      p <- p + 
        geom_density_2d(data = plotdata[[sf]]
                        , mapping = aes_string(x = d_cluster_val
                                               , y = 'prop'
                                               , colour = 'source'
                                               , fill = 'source'
                                               )
                        , alpha = 0.5
                        ) 
    }

    ## return plot object    
    p
  } 
  
plot_cluster_parameters(stan_samples_dp, restaurant)

```

Depending on the sample of data used to fit this model, it may or may not fit that well. There are some fundamental problems with this model, since the Stan code was adapted from that used to fit the mixture model, which we will address shortly.

In the meantime, the real value of the DP model is not having to specify the number of groups. Let's see what happens with the DP model if we increase the number of groups to, say, K = 10.

```{r fit-dp-model-k10, eval = T}
stan_samples_dp_k10 <- stan(model_code = model_string_with_dp,
                     iter = iter*2,
                     seed = seed,
                     data = list(y = restaurant$rescaled_value,
                                 n = nrow(restaurant),
                                 K = 10),
                     control = list(adapt_delta = 0.99, max_treedepth = 15),
                     model_name = 'Simple mixture model with Dirichlet prior'
                     )

print(stan_samples_dp_k10, pars = c('theta'))

```

In this case, we're a lot less interested in what happens to the thetas than the proportions.

```{r dp-model-proportions}
print(stan_samples_dp_k10, pars = c('proportions'))
```

And, let's compare the combination of proportion & theta to our original data.

```{r compare-dp-estimates-theta}
plot_cluster_parameters(stan_samples_dp_k10, restaurant)
```

Let's compare this to the same computation using the simple mixture model with K = 4

```{r compare-dp-k4-estimates-thetas}
plot_cluster_parameters(stanfit = c('DP (5) model' = stan_samples_dp
                                    , 'DP (10) model' = stan_samples_dp_k10
                                    #, 'MM model' = stan_samples_k4
                                    )
                        , d = restaurant)

```

## Improving the model by unconstraining theta

There is one modification we need to make to our model since it imposes a constraint that may not be necessary (or tenable)..

Namely, the original mixture model forced the thetas to be "ordered" -- without this constraint all clusters would be exchangeable & so we'd have too much class switching (we still have class switching but less of it). 

In this model, forcing the *means* to be ordered is problematic because we have additionally imposed some order on the *size* of each cluster, via the prior on the proportions parameter. There is no reason to think that the largest clusters will also have the smallest means, and so we should now relax one of these two constraints.

Let's see how well this revised model fits our data.

```{r define-dp2-DPP-mod-theta-prior}
model_string_with_dp2 <- "
data {
  int n;
  real y[n];
  int K; ## presumed number of groups
}

parameters {
  simplex[K] proportions;  ## mixing proportion (pis)
  vector[K] theta;           ## means for each group
  real<lower=0> sigma;     ## overall sigma; does not vary by group
  real<lower=0> a;         ## concentration parameter
}

transformed parameters {
  vector<lower=0>[K] alpha;
  for (k in 1:K) {
    alpha[k] <- a/k;
  }
}

model {
  ## priors on parameters
  theta ~ normal(0, 1);
  a ~ normal(10, 10);
  proportions ~ dirichlet(alpha);
  
  ## compute likelihood for each observation, integrating out discrete-ness of groups
  for (i in 1:n) {
    real ps[K]; ## temp holder for log component densities
    for (k in 1:K) {
      ps[k] <- log(proportions[k]) + normal_log(y[i], theta[k], sigma);
    }
    increment_log_prob(log_sum_exp(ps));
  }
}

generated quantities {
  real log_lik[n];
  
  for (i in 1:n) {
    real ps[K]; ## temp holder for log component densities
    for (k in 1:K) {
      ps[k] <- log(proportions[k]) + normal_log(y[i], theta[k], sigma);
    }
    log_lik[i] <- log_sum_exp(ps);
  }
}
"
```

### Fitting adustable-prior model to data 

Try fitting this revised model to our data. For now, start with K = 10 since that's what we did previously.

```{r fit-dp2-model, eval = T}
stan_samples_dp2 <- stan(model_code = model_string_with_dp2
                         , iter = iter*2
                         , seed = seed
                         , data = list(y = restaurant$rescaled_value
                                       , n = nrow(restaurant)
                                       , K = 10
                                       )
                         , control = list(adapt_delta = 0.99, max_treedepth = 15)
                         , model_name = 'Gaussian mixture model with Dirichlet prior (v2)'
                         )

## look at thetas
print(stan_samples_dp2, pars = c('theta'))

```

Note also that we have increased the adapt delta & max_treedepth yet again, based on repeated runs of this model.

### Comparing estimated thetas to known truth

Now let's compare the distribution of thetas from this revised model to the ground truth.

```{r compare-dp2-estimates-theta}

plot_cluster_parameters(stanfit = c('revised model' = stan_samples_dp2)
                        , d = restaurant
                        )

```

This yields a very similar, although not identical fit. 

Let's see how Loo ranks these two models.

```{r compare-dp1-dp2-loo}
loo_dp2 <- loo(rstan::extract(stan_samples_dp2,'log_lik')$log_lik)
loo_dp_k10 <- loo(rstan::extract(stan_samples_dp_k10,'log_lik')$log_lik)
loo::compare(loo_dp, loo_dp2, loo_dp_k10, loo_k4)
```

It also seems like we want to have an even more strongly diminishing likelihood of new clusters - maybe put a stronger prior on the `{r, eval = F} a` parameter.

## Improving the model by adjusting priors on a and theta

Now we're going to modify the model again by removing priors on theta & modifying the prior on a to be stronger.

Recall that we set `{r eval = F} a` to `{r eval = T} a` initially. How close is this to our estimate?

```{r compare-dp2-estimates-a}

print(stan_samples_dp2, pars = 'a')

estimated_a_dp2 <- get_sampled_parameter(stan_samples_dp2, 'a')
ggplot() +
  geom_density(data = estimated_a_dp2
               , aes(x = log1p(parameter_value), colour = 'estimated values')) + 
  geom_vline(aes(xintercept = log1p(1), colour = 'true value'))
```

Hopefully the true value is within the range of estimates drawn from the model. Chances are, the range of values estimated is still quite broad.

Let's write a revised version of this model, so that we can modify the priors on a (and, while we're at it, the priors on theta).

```{r define-dp3-DPP-mod-a-prior}
model_string_with_dp3 <- "
data {
  int n;
  real y[n];
  int K; ## presumed number of groups
  real prior_theta_mean;
  real prior_theta_sd;
  real prior_a_mean;
  real prior_a_sd;
}

parameters {
  simplex[K] proportions;  ## mixing proportion (pis)
  vector[K] theta;         ## means for each group
  real<lower=0> sigma;     ## overall sigma; does not vary by group
  real<lower=0> a;         ## concentration parameter
}

transformed parameters {
  vector<lower=0>[K] alpha;
  for (k in 1:K) {
    alpha[k] <- a/k;
  }
}

model {
  ## priors on parameters
  theta ~ normal(prior_theta_mean, prior_theta_sd);
  a ~ normal(prior_a_mean, prior_a_sd);
  proportions ~ dirichlet(alpha);
  
  ## compute likelihood for each observation, integrating out discrete-ness of groups
  for (i in 1:n) {
    real ps[K]; ## temp holder for log component densities
    for (k in 1:K) {
      ps[k] <- log(proportions[k]) + normal_log(y[i], theta[k], sigma);
    }
    increment_log_prob(log_sum_exp(ps));
  }
}

generated quantities {
  real log_lik[n];
  
  for (i in 1:n) {
    real ps[K]; ## temp holder for log component densities
    for (k in 1:K) {
      ps[k] <- log(proportions[k]) + normal_log(y[i], theta[k], sigma);
    }
    log_lik[i] <- log_sum_exp(ps);
  }
}
"
```

### Fitting revised model

```{r fit-dp3-model, eval = T}
stan_samples_dp3 <- stan(model_code = model_string_with_dp3
                         , iter = iter*2
                         , seed = seed
                         , data = list(y = restaurant$rescaled_value
                                       , n = nrow(restaurant)
                                       , K = 10
                                       , prior_theta_mean = 0
                                       , prior_theta_sd = 1
                                       , prior_a_mean = 0
                                       , prior_a_sd = 1
                                       )
                         , control = list(adapt_delta = 0.98, max_treedepth = 15)
                         , model_name = 'Guassian mixture model with Dirichlet prior (v3)'
                         )

## look at thetas
print(stan_samples_dp3, pars = c('theta','a'))

```

### comparing estimates of theta to known values 

Now, repeating the process above, we look at similarity of thetas to known values (weighted by proportion)

```{r compare-dp3-estimates-theta}

estimated_prop_dp3 <- get_sampled_parameter(stan_samples_dp3, 'proportions', parameter_id = 'cluster_id', parameter_value = 'prop')
estimated_theta_dp3 <- get_sampled_parameter(stan_samples_dp3, 'theta', parameter_id = 'cluster_id', parameter_value = 'theta_scaled')

estimated_dp3 <- 
  estimated_theta_dp3 %>%
  inner_join(estimated_prop_dp3
             , by = c('cluster_id','iter')
             )

ggplot() +
  geom_point(data = true_theta
             , mapping = aes(x = theta_scaled, y = prop, size = prop, colour = 'true clusters', fill = 'true clusters')
             ) +
  scale_fill_discrete(guide = "none") +
  scale_size_continuous(guide = "none") +
  geom_density_2d(data = estimated_dp3, mapping = aes(x = theta_scaled, y = prop, colour = 'estimated - DP model')) +
  theme_bw()
```

### comparing estimates of a to known value

And, look again at the distribution of values of a.

```{r compare-dp3-estimates-a}

estimated_a_dp3 <- get_sampled_parameter(stan_samples_dp3, 'a')
ggplot() +
  geom_density(data = estimated_a_dp3, aes(x = log1p(parameter_value), colour = 'estimated values')) + 
  geom_vline(aes(xintercept = log1p(a), colour = 'true value'))
```

In general, it seems like we have too little data & thus our estimates are heavily influenced by our priors.

### Comparing this model's performance to the k4 model

```{r compare-dp3-to-k4}
loo_dp3 <- loo(rstan::extract(stan_samples_dp3, 'log_lik')$log_lik)
loo::compare(loo_dp3, loo_k4)
```

At this point, the k4 model is only marginally better than this "dp3" model.

Just for kicks, let's try a uniform prior on the thetas to see how this model does.

## Model with uniform prior on thetas

Now we're going to try a similar model but remove the prior on the thetas to be basically non-informative.

For sake of sampling efficiency, we will want to bound the values of theta so that the sampler doesn't have to explore the entire sample space -- without a prior, values as large as 1000 as just as plausible as those close to 0. 

This is not an optimal approach in general, but in our case isn't terrible since we will be applying these data to a constrained [0,1] sample space in practice. 

Here is the revised model code:

```{r model-definition-dp4-no-theta-prior}
model_string_with_dp4 <- "
data {
  int n;
  real y[n];
  int K; ## presumed number of groups
  real prior_a_mean;
  real prior_a_sd;
}

parameters {
  simplex[K] proportions;  ## mixing proportion (pis)
  vector<lower=-10,upper=10>[K] theta;           ## means for each group
  real<lower=0> sigma;     ## overall sigma; does not vary by group
  real<lower=0> a;         ## concentration parameter
}

transformed parameters {
  vector<lower=0>[K] alpha;
  for (k in 1:K) {
    alpha[k] <- a/k;
  }
}

model {
  ## priors on parameters
  a ~ normal(prior_a_mean, prior_a_sd);
  proportions ~ dirichlet(alpha);
  
  ## compute likelihood for each observation, integrating out discrete-ness of groups
  for (i in 1:n) {
    real ps[K]; ## temp holder for log component densities
    for (k in 1:K) {
      ps[k] <- log(proportions[k]) + normal_log(y[i], theta[k], sigma);
    }
    increment_log_prob(log_sum_exp(ps));
  }
}

generated quantities {
  real log_lik[n];
  
  for (i in 1:n) {
    real ps[K]; ## temp holder for log component densities
    for (k in 1:K) {
      ps[k] <- log(proportions[k]) + normal_log(y[i], theta[k], sigma);
    }
    log_lik[i] <- log_sum_exp(ps);
  }
}
"
```

### Fitting revised model

Next we fit this model to data, leaving most of the parameters as before.

```{r fit-dp4-model, eval = T}
stan_samples_dp4 <- stan(model_code = model_string_with_dp4
                         , iter = iter*2
                         , seed = seed
                         , data = list(y = restaurant$rescaled_value
                                       , n = nrow(restaurant)
                                       , K = 10
                                       , prior_a_mean = 0
                                       , prior_a_sd = 1
                                       )
                         , control = list(adapt_delta = 0.98, max_treedepth = 15)
                         , model_name = 'Simple mixture model with Dirichlet prior'
                         )

## look at thetas
print(stan_samples_dp4, pars = c('theta','a'))

```

### comparing estimates of theta to known values 

Finally, we repeat the process above once again to look at similarity of thetas to known values (weighted by proportion)

```{r compare-dp4-estimates-theta}

estimated_prop_dp4 <- get_sampled_parameter(stan_samples_dp4, 'proportions', parameter_id = 'cluster_id', parameter_value = 'prop')
estimated_theta_dp4 <- get_sampled_parameter(stan_samples_dp4, 'theta', parameter_id = 'cluster_id', parameter_value = 'theta_scaled')

estimated_dp4 <- 
  estimated_theta_dp4 %>%
  inner_join(estimated_prop_dp4
             , by = c('cluster_id','iter')
             )

ggplot() +
  geom_point(data = true_theta
             , mapping = aes(x = theta_scaled, y = prop, size = prop, colour = 'true clusters', fill = 'true clusters')
             ) +
  scale_fill_discrete(guide = "none") +
  scale_size_continuous(guide = "none") +
  geom_density_2d(data = estimated_dp4, mapping = aes(x = theta_scaled, y = prop, colour = 'estimated - DP model')) +
  theme_bw()
```

Appears that even a uniform prior on the thetas is "informative".

### comparing estimates of a to known value

Let's see how this modifies the estimates of values of a.

```{r compare-dp4-estimates-a}

estimated_a_dp4 <- get_sampled_parameter(stan_samples_dp4, 'a')
ggplot() +
  geom_density(data = estimated_a_dp4, aes(x = log1p(parameter_value), colour = 'estimated values')) + 
  geom_vline(aes(xintercept = log1p(a), colour = 'true value'))
```

Interesting that this model seems to perform much more poorly than that with a more informative prior on thetas.

### Comparing this model's performance to the k4 model

Let's see how this impacts our "loo" estimates.

```{r compare-dp4-to-k4}
loo_dp4 <- loo(rstan::extract(stan_samples_dp4, 'log_lik')$log_lik)
loo::compare(loo_dp4, loo_k4)
```

Definitely worse than the K4 model.

Is it measureably worse than the dp3 model?

```{r compare-dp4-to-dp3}
loo::compare(loo_dp4, loo_dp3)
```

Pretty much.

## Summarizing what we've learned

At this point we've implemented several iterations of two models to fit some data generated via a Chinese Resaurant Process 

These models are (broadly):

1. a simple mixture model (MM)
2. a semi-parametric mixture model with Dirichlet Process (DP) priors

Observations :

1. For these data (with sample size of `{r} nrow(restaurant)`), the MM and DP models fit our data +/- equally as well.
    * We don't yet know if the difference will be more apparent at different (ie larger) sample sizes
2. In the DP model, we end up proposing many more clusters than we expect will exist, leaving vestigial groups 
    * May consider a penalized likelihood so that the model prefers a smaller number of groups, all things being equal
3. Most remaining issues are common to both models:
    * Relatively strong influence of priors on the result
    * Label-switching among clusters; this is still a problem although minimized by parameterization

Next steps:

1. Re-run analysis with a larger sample size (10k instead of 100)
2. Look at implementation of TSSB , instead of DP



